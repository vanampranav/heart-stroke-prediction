{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978ebc6-ee6a-45ba-8413-bcfadb2ac0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.69.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vanamabhinav/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m481.8/615.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:36\u001b[0m:38\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Standardize dataset\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Particle Swarm Optimization (PSO) parameters\n",
    "SWARM_SIZE = 10\n",
    "GENERATIONS = 5\n",
    "W = 0.5  # Inertia weight\n",
    "C1 = 1.5  # Cognitive (personal) weight\n",
    "C2 = 1.5  # Social (group) weight\n",
    "\n",
    "def evaluate_model(individual):\n",
    "    \"\"\"Evaluate a model based on an individual's parameters.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(individual[0], input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(individual[1]),\n",
    "        Dense(individual[2], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(individual[3]),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=individual[4]), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=individual[5], verbose=0, validation_split=0.2)\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def initialize_particles():\n",
    "    \"\"\"Generate an initial swarm of particles.\"\"\"\n",
    "    particles = [\n",
    "        [random.randint(10, 100),  # First layer neurons\n",
    "         random.uniform(0.1, 0.5),  # First layer dropout\n",
    "         random.randint(10, 100),  # Second layer neurons\n",
    "         random.uniform(0.1, 0.5),  # Second layer dropout\n",
    "         random.uniform(0.0001, 0.01),  # Learning rate\n",
    "         random.randint(16, 128)]  # Batch size\n",
    "        for _ in range(SWARM_SIZE)\n",
    "    ]\n",
    "    velocities = [\n",
    "        [random.uniform(-1, 1) for _ in range(len(particles[0]))]\n",
    "        for _ in range(SWARM_SIZE)\n",
    "    ]\n",
    "    return particles, velocities\n",
    "\n",
    "# PSO optimization\n",
    "particles, velocities = initialize_particles()\n",
    "personal_best = particles.copy()\n",
    "global_best = max(particles, key=evaluate_model)\n",
    "personal_best_scores = [evaluate_model(p) for p in particles]\n",
    "\n",
    "def update_velocity(velocity, particle, personal_best, global_best):\n",
    "    \"\"\"Update the velocity of a particle.\"\"\"\n",
    "    new_velocity = []\n",
    "    for i in range(len(particle)):\n",
    "        r1, r2 = random.random(), random.random()\n",
    "        cognitive = C1 * r1 * (personal_best[i] - particle[i])\n",
    "        social = C2 * r2 * (global_best[i] - particle[i])\n",
    "        new_velocity.append(W * velocity[i] + cognitive + social)\n",
    "    return new_velocity\n",
    "\n",
    "def update_particle(particle, velocity):\n",
    "    \"\"\"Update the position of a particle.\"\"\"\n",
    "    new_particle = [\n",
    "        max(10, min(100, int(particle[0] + velocity[0]))),\n",
    "        max(0.1, min(0.5, particle[1] + velocity[1])),\n",
    "        max(10, min(100, int(particle[2] + velocity[2]))),\n",
    "        max(0.1, min(0.5, particle[3] + velocity[3])),\n",
    "        max(0.0001, min(0.01, particle[4] + velocity[4])),\n",
    "        max(16, min(128, int(particle[5] + velocity[5])))\n",
    "    ]\n",
    "    return new_particle\n",
    "\n",
    "for generation in range(GENERATIONS):\n",
    "    for i in range(SWARM_SIZE):\n",
    "        fitness = evaluate_model(particles[i])\n",
    "        if fitness > personal_best_scores[i]:\n",
    "            personal_best_scores[i] = fitness\n",
    "            personal_best[i] = particles[i]\n",
    "        if fitness > evaluate_model(global_best):\n",
    "            global_best = particles[i]\n",
    "        velocities[i] = update_velocity(velocities[i], particles[i], personal_best[i], global_best)\n",
    "        particles[i] = update_particle(particles[i], velocities[i])\n",
    "\n",
    "    print(f\"Generation {generation + 1}, Best Accuracy: {evaluate_model(global_best)}\")\n",
    "\n",
    "# Best solution\n",
    "best_individual = global_best\n",
    "print(\"Best Individual:\", best_individual)\n",
    "\n",
    "# Train the best model\n",
    "best_model = Sequential([\n",
    "    Dense(best_individual[0], input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(best_individual[1]),\n",
    "    Dense(best_individual[2], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(best_individual[3]),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "best_model.compile(optimizer=Adam(learning_rate=best_individual[4]), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "best_model.fit(X_train, y_train, epochs=20, batch_size=best_individual[5], verbose=0)\n",
    "\n",
    "# Extract features for Random Forest\n",
    "train_features = best_model.predict(X_train)\n",
    "test_features = best_model.predict(X_test)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(train_features, y_train)\n",
    "\n",
    "# Predictions and classification report\n",
    "rf_predictions = rf_model.predict(test_features)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0796692-2614-4205-90f0-1c6466de8c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
